# Paramètres du LLM

Lorsque vous travaillez avec des invites, vous interagissez avec le LLM via une API ou directement. Vous pouvez configurer quelques paramètres pour obtenir des résultats différents pour vos invites. 

**Température** - En bref, plus la température est basse, plus les résultats sont déterministes dans le sens où le prochain jeton le plus probable est toujours choisi. L'augmentation de la température peut conduire à plus d'aléatoire, encourageant des résultats plus diversifiés ou créatifs. Nous augmentons essentiellement le poids des autres jetons possibles. En termes d'application, nous pourrions vouloir utiliser une valeur de température plus basse pour des tâches telles que l'assurance qualité basée sur les faits afin d'encourager des réponses plus factuelles et concises. Pour la création de poèmes ou d'autres tâches créatives, il peut être avantageux d'augmenter la valeur de la température.

**Top_p** - De même, avec `top_p`, une technique d'échantillonnage avec température appelée échantillonnage de noyau, vous pouvez contrôler le degré de déterminisme avec lequel le modèle génère une réponse. Si vous recherchez des réponses exactes et factuelles, gardez cette valeur basse. Si vous recherchez des réponses plus diversifiées, augmentez la valeur. 

La recommandation générale est de modifier l'un des deux, pas les deux.

Avant de commencer avec quelques exemples de base, gardez à l'esprit que vos résultats peuvent varier en fonction de la version de LLM que vous utilisez.